{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LEGION\\anaconda3\\envs\\indicwiki\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import anuvaad\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from anuvaad import Anuvaad\n",
    "anu = Anuvaad(\"english-telugu\")\n",
    "\n",
    "data = pd.read_pickle('data_v5_habitat_anuvaad.pkl')\n",
    "def create_telugu_col(col = 'Binomial Name',data=data):\n",
    "    data[col+ ' Telugu'] = np.nan\n",
    "    for i in tqdm(range(data.shape[0])):\n",
    "        # print(i)\n",
    "        if type(data[col][i]) == float:\n",
    "            data[col + ' Telugu'][i] = np.nan\n",
    "        else:\n",
    "            data[col + ' Telugu'][i] = anu.anuvaad(data[col][i])\n",
    "    return data\n",
    "# data = pd.read_pickle('data_v5_habitat_anuvaad.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11099 [00:00<?, ?it/s]C:\\Users\\LEGION\\AppData\\Local\\Temp\\ipykernel_22064\\2992456056.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[col + ' Telugu'][i] = anu.anuvaad(data[col][i])\n",
      " 27%|██▋       | 2951/11099 [9:23:58<25:57:11, 11.47s/it]   \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\LEGION\\Documents\\birds\\Telugu_trans\\Translate_v2.ipynb Cell 2'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/LEGION/Documents/birds/Telugu_trans/Translate_v2.ipynb#ch0000001?line=0'>1</a>\u001b[0m data \u001b[39m=\u001b[39m create_telugu_col(col\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mFamily\u001b[39;49m\u001b[39m'\u001b[39;49m,data\u001b[39m=\u001b[39;49mdata)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/LEGION/Documents/birds/Telugu_trans/Translate_v2.ipynb#ch0000001?line=1'>2</a>\u001b[0m data\u001b[39m.\u001b[39mto_pickle(\u001b[39m'\u001b[39m\u001b[39mdata_v7_Family_anuvaad.pkl\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/LEGION/Documents/birds/Telugu_trans/Translate_v2.ipynb#ch0000001?line=2'>3</a>\u001b[0m data\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39mdata_v7_Family_anuvaad.csv\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\LEGION\\Documents\\birds\\Telugu_trans\\Translate_v2.ipynb Cell 1'\u001b[0m in \u001b[0;36mcreate_telugu_col\u001b[1;34m(col, data)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/LEGION/Documents/birds/Telugu_trans/Translate_v2.ipynb#ch0000000?line=13'>14</a>\u001b[0m         data[col \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m Telugu\u001b[39m\u001b[39m'\u001b[39m][i] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mnan\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/LEGION/Documents/birds/Telugu_trans/Translate_v2.ipynb#ch0000000?line=14'>15</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/LEGION/Documents/birds/Telugu_trans/Translate_v2.ipynb#ch0000000?line=15'>16</a>\u001b[0m         data[col \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m Telugu\u001b[39m\u001b[39m'\u001b[39m][i] \u001b[39m=\u001b[39m anu\u001b[39m.\u001b[39;49manuvaad(data[col][i])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/LEGION/Documents/birds/Telugu_trans/Translate_v2.ipynb#ch0000000?line=16'>17</a>\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\indicwiki\\lib\\site-packages\\anuvaad\\anuvaad.py:183\u001b[0m, in \u001b[0;36mAnuvaad.anuvaad\u001b[1;34m(self, sentences, source_lang, target_lang, beam_size, max_len)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/anuvaad/anuvaad.py?line=179'>180</a>\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n\u001b[0;32m    <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/anuvaad/anuvaad.py?line=180'>181</a>\u001b[0m     input_ids \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/anuvaad/anuvaad.py?line=182'>183</a>\u001b[0m output_ids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerate(\n\u001b[0;32m    <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/anuvaad/anuvaad.py?line=183'>184</a>\u001b[0m     input_ids, num_beams\u001b[39m=\u001b[39;49mbeam_size, max_length\u001b[39m=\u001b[39;49mmax_len\n\u001b[0;32m    <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/anuvaad/anuvaad.py?line=184'>185</a>\u001b[0m )\n\u001b[0;32m    <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/anuvaad/anuvaad.py?line=186'>187</a>\u001b[0m outputs \u001b[39m=\u001b[39m [\n\u001b[0;32m    <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/anuvaad/anuvaad.py?line=187'>188</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mdecode(output_id, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/anuvaad/anuvaad.py?line=188'>189</a>\u001b[0m     \u001b[39mfor\u001b[39;00m output_id \u001b[39min\u001b[39;00m output_ids\n\u001b[0;32m    <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/anuvaad/anuvaad.py?line=189'>190</a>\u001b[0m ]\n\u001b[0;32m    <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/anuvaad/anuvaad.py?line=191'>192</a>\u001b[0m \u001b[39mif\u001b[39;00m return_single:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\indicwiki\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/torch/autograd/grad_mode.py?line=23'>24</a>\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/torch/autograd/grad_mode.py?line=24'>25</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/torch/autograd/grad_mode.py?line=25'>26</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/torch/autograd/grad_mode.py?line=26'>27</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\indicwiki\\lib\\site-packages\\transformers\\generation_utils.py:1315\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, **model_kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/generation_utils.py?line=1310'>1311</a>\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/generation_utils.py?line=1311'>1312</a>\u001b[0m         input_ids, expand_size\u001b[39m=\u001b[39mnum_beams, is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/generation_utils.py?line=1312'>1313</a>\u001b[0m     )\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/generation_utils.py?line=1313'>1314</a>\u001b[0m     \u001b[39m# 12. run beam search\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/generation_utils.py?line=1314'>1315</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeam_search(\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/generation_utils.py?line=1315'>1316</a>\u001b[0m         input_ids,\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/generation_utils.py?line=1316'>1317</a>\u001b[0m         beam_scorer,\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/generation_utils.py?line=1317'>1318</a>\u001b[0m         logits_processor\u001b[39m=\u001b[39mlogits_processor,\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/generation_utils.py?line=1318'>1319</a>\u001b[0m         stopping_criteria\u001b[39m=\u001b[39mstopping_criteria,\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/generation_utils.py?line=1319'>1320</a>\u001b[0m         pad_token_id\u001b[39m=\u001b[39mpad_token_id,\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/generation_utils.py?line=1320'>1321</a>\u001b[0m         eos_token_id\u001b[39m=\u001b[39meos_token_id,\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/generation_utils.py?line=1321'>1322</a>\u001b[0m         output_scores\u001b[39m=\u001b[39moutput_scores,\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/generation_utils.py?line=1322'>1323</a>\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39mreturn_dict_in_generate,\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/generation_utils.py?line=1323'>1324</a>\u001b[0m         synced_gpus\u001b[39m=\u001b[39msynced_gpus,\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/generation_utils.py?line=1324'>1325</a>\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/generation_utils.py?line=1325'>1326</a>\u001b[0m     )\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/generation_utils.py?line=1327'>1328</a>\u001b[0m \u001b[39melif\u001b[39;00m is_beam_sample_gen_mode:\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/generation_utils.py?line=1328'>1329</a>\u001b[0m     \u001b[39m# 10. prepare logits warper\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/generation_utils.py?line=1329'>1330</a>\u001b[0m     logits_warper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/generation_utils.py?line=1330'>1331</a>\u001b[0m         top_k\u001b[39m=\u001b[39mtop_k, top_p\u001b[39m=\u001b[39mtop_p, typical_p\u001b[39m=\u001b[39mtypical_p, temperature\u001b[39m=\u001b[39mtemperature, num_beams\u001b[39m=\u001b[39mnum_beams\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/generation_utils.py?line=1331'>1332</a>\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\indicwiki\\lib\\site-packages\\transformers\\generation_utils.py:2158\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[1;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/generation_utils.py?line=2153'>2154</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/generation_utils.py?line=2155'>2156</a>\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m-> <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/generation_utils.py?line=2157'>2158</a>\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/generation_utils.py?line=2158'>2159</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_inputs,\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/generation_utils.py?line=2159'>2160</a>\u001b[0m     return_dict\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/generation_utils.py?line=2160'>2161</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/generation_utils.py?line=2161'>2162</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/generation_utils.py?line=2162'>2163</a>\u001b[0m )\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/generation_utils.py?line=2164'>2165</a>\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/generation_utils.py?line=2165'>2166</a>\u001b[0m     cur_len \u001b[39m=\u001b[39m cur_len \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\indicwiki\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\indicwiki\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1638\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1634'>1635</a>\u001b[0m         decoder_attention_mask \u001b[39m=\u001b[39m decoder_attention_mask\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder\u001b[39m.\u001b[39mfirst_device)\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1636'>1637</a>\u001b[0m \u001b[39m# Decode\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1637'>1638</a>\u001b[0m decoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1638'>1639</a>\u001b[0m     input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1639'>1640</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1640'>1641</a>\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1641'>1642</a>\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1642'>1643</a>\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1643'>1644</a>\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1644'>1645</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mdecoder_head_mask,\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1645'>1646</a>\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39;49mcross_attn_head_mask,\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1646'>1647</a>\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1647'>1648</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1648'>1649</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1649'>1650</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1650'>1651</a>\u001b[0m )\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1652'>1653</a>\u001b[0m sequence_output \u001b[39m=\u001b[39m decoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1654'>1655</a>\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\indicwiki\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\indicwiki\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1033\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1019'>1020</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m checkpoint(\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1020'>1021</a>\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1021'>1022</a>\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1029'>1030</a>\u001b[0m         \u001b[39mNone\u001b[39;00m,  \u001b[39m# past_key_value is always None with gradient checkpointing\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1030'>1031</a>\u001b[0m     )\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1031'>1032</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1032'>1033</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1033'>1034</a>\u001b[0m         hidden_states,\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1034'>1035</a>\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1035'>1036</a>\u001b[0m         position_bias\u001b[39m=\u001b[39;49mposition_bias,\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1036'>1037</a>\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1037'>1038</a>\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1038'>1039</a>\u001b[0m         encoder_decoder_position_bias\u001b[39m=\u001b[39;49mencoder_decoder_position_bias,\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1039'>1040</a>\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1040'>1041</a>\u001b[0m         cross_attn_layer_head_mask\u001b[39m=\u001b[39;49mcross_attn_layer_head_mask,\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1041'>1042</a>\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1042'>1043</a>\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1043'>1044</a>\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1044'>1045</a>\u001b[0m     )\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1046'>1047</a>\u001b[0m \u001b[39m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1047'>1048</a>\u001b[0m \u001b[39m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\indicwiki\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\indicwiki\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:720\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[1;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/models/t5/modeling_t5.py?line=716'>717</a>\u001b[0m     attention_outputs \u001b[39m=\u001b[39m attention_outputs \u001b[39m+\u001b[39m cross_attention_outputs[\u001b[39m2\u001b[39m:]\n\u001b[0;32m    <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/models/t5/modeling_t5.py?line=718'>719</a>\u001b[0m \u001b[39m# Apply Feed Forward layer\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/models/t5/modeling_t5.py?line=719'>720</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m](hidden_states)\n\u001b[0;32m    <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/models/t5/modeling_t5.py?line=721'>722</a>\u001b[0m \u001b[39m# clamp inf values to enable fp16 training\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/models/t5/modeling_t5.py?line=722'>723</a>\u001b[0m \u001b[39mif\u001b[39;00m hidden_states\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m torch\u001b[39m.\u001b[39mfloat16 \u001b[39mand\u001b[39;00m torch\u001b[39m.\u001b[39misinf(hidden_states)\u001b[39m.\u001b[39many():\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\indicwiki\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\indicwiki\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:328\u001b[0m, in \u001b[0;36mT5LayerFF.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/models/t5/modeling_t5.py?line=325'>326</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states):\n\u001b[0;32m    <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/models/t5/modeling_t5.py?line=326'>327</a>\u001b[0m     forwarded_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm(hidden_states)\n\u001b[1;32m--> <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/models/t5/modeling_t5.py?line=327'>328</a>\u001b[0m     forwarded_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mDenseReluDense(forwarded_states)\n\u001b[0;32m    <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/models/t5/modeling_t5.py?line=328'>329</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m hidden_states \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(forwarded_states)\n\u001b[0;32m    <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/models/t5/modeling_t5.py?line=329'>330</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\indicwiki\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\indicwiki\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:304\u001b[0m, in \u001b[0;36mT5DenseGatedGeluDense.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/models/t5/modeling_t5.py?line=301'>302</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states):\n\u001b[0;32m    <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/models/t5/modeling_t5.py?line=302'>303</a>\u001b[0m     hidden_gelu \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgelu_act(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwi_0(hidden_states))\n\u001b[1;32m--> <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/models/t5/modeling_t5.py?line=303'>304</a>\u001b[0m     hidden_linear \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwi_1(hidden_states)\n\u001b[0;32m    <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/models/t5/modeling_t5.py?line=304'>305</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m hidden_gelu \u001b[39m*\u001b[39m hidden_linear\n\u001b[0;32m    <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/transformers/models/t5/modeling_t5.py?line=305'>306</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\indicwiki\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\indicwiki\\lib\\site-packages\\torch\\nn\\modules\\linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/torch/nn/modules/linear.py?line=101'>102</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> <a href='file:///c%3A/Users/LEGION/anaconda3/envs/indicwiki/lib/site-packages/torch/nn/modules/linear.py?line=102'>103</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = create_telugu_col(col='Family',data=data)\n",
    "data.to_pickle('data_v7_Family_anuvaad.pkl')\n",
    "data.to_csv('data_v7_Family_anuvaad.csv')\n",
    "data.to_excel('data_v7_Family_anuvaad.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "# from anuvaad import Anuvaad\n",
    "# anu = Anuvaad(\"english-telugu\")\n",
    "\n",
    "# data = pd.read_pickle('data_v5_habitat_anuvaad.pkl')\n",
    "# def create_telugu_col_new_optim(col,data):\n",
    "#     di = {}\n",
    "#     data[col+ ' Telugu'] = np.nan\n",
    "#     for i in tqdm(range(data.shape[0])):\n",
    "#         # print(i)\n",
    "#         if type(data[col][i]) == float:\n",
    "#             data[col + ' Telugu'][i] = np.nan\n",
    "#         else:\n",
    "#             if data[col][i] in di:\n",
    "#                 data[col + ' Telugu'][i] = di[data[col][i]]\n",
    "#             else:\n",
    "#                 data[col + ' Telugu'][i] = anu.anuvaad(data[col][i])\n",
    "#                 di[data[col][i]] = data[col + ' Telugu'][i]\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = create_telugu_col_new_optim('Country',data=data)\n",
    "# data.to_pickle('data_v9_Country_anuvaad.pkl')\n",
    "# data.to_csv('data_v9_Country_anuvaad.csv')\n",
    "# data.to_excel('data_v9_Country_anuvaad.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Breeding region\n",
    "# data = create_telugu_col_new_optim('Breeding region',data=data)\n",
    "# data.to_pickle('data_v2_2_Breeding_region_anuvaad.pkl')\n",
    "# data.to_csv('data_v2_2_Breeding_region_anuvaad.csv')\n",
    "# data.to_excel('data_v2_2_Breeding_region_anuvaad.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AkWiki",
   "language": "python",
   "name": "indicwiki"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
